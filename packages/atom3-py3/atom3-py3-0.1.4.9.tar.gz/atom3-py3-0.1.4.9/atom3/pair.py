import collections as col
import logging
import multiprocessing as mp
import os

import dill
import numpy as np
import pandas as pd
import parallel as par

import atom3.case as ca
import atom3.complex as comp
import atom3.database as db
import atom3.neighbors as nb

Pair = col.namedtuple(
    'Pair', ['complex', 'df0', 'df1', 'pos_heavy_atom_idxs', 'srcs', 'id'])

sem = mp.Semaphore()


def add_pairs_parser(subparsers, pp):
    """Add parser."""

    def all_complexes_to_pairs_main(args):
        all_complexes_to_pairs_full(args)

    ap = subparsers.add_parser(
        'pairs', description='complexes to pairs',
        help='uses output of the complex command to split pickled proteins',
        parents=[pp])
    ap.set_defaults(func=all_complexes_to_pairs_main)
    ap.add_argument('complexes_dill', metavar='complexes.dill', type=str,
                    help='complexes file')
    ap.add_argument('output_dir', type=str,
                    help='directory to output to')
    ap.add_argument('-n', '--criteria', dest='criteria',
                    choices=['ca', 'heavy'],
                    default='ca', help='criteria for finding neighboring'
                                       ' residues (default: by alpha carbon distance)')
    ap.add_argument('-t', '--cutoff', dest='cutoff', type=float,
                    default=8, help='cutoff distance to be used with'
                                    ' neighbor criteria (default: 8)')
    ap.add_argument('-f', '--full', dest='full', action='store_true',
                    help='generate all possible negative examples, '
                         'as opposed to a sampling of same size as positive.')
    ap.add_argument(
        '-u', '--unbound', help='whether to use unbound data.',
        action="store_true")
    ap.add_argument('-c', metavar='cpus', default=mp.cpu_count(), type=int,
                    help='number of cpus to use for processing (default:'
                         ' number processors available on current machine)')


def all_complexes_to_pairs_full(args):
    complexes = comp.read_complexes(args.complexes_dill)
    get_neighbors = nb.build_get_neighbors(args.criteria, args.cutoff)
    get_pairs = build_get_pairs(complexes['type'], args.unbound, get_neighbors,
                                args.full)
    all_complex_to_pairs(complexes, get_pairs, args.output_dir, args.c)


def all_complex_to_pairs(complexes, get_pairs, output_dir, num_cpus):
    """Reads in structures and produces appropriate pairings."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    requested_keys = complexes['data'].keys()
    produced_keys = complexes_from_pair_dir(output_dir)
    work_keys = [key for key in requested_keys if key not in produced_keys]

    inputs = [(complexes['data'][key], get_pairs, output_dir)
              for key in work_keys]
    logging.info("{:} requested keys, {:} produced keys, {:} work keys"
                 .format(len(requested_keys), len(produced_keys),
                         len(work_keys)))
    par.submit_jobs(complex_to_pairs, inputs, num_cpus)


def complexes_from_pair_dir(pair_dir):
    """Get all complex names from provided pair directory."""
    filenames = db.get_structures_filenames(pair_dir, extension='.dill')
    # Remove per-chain identifier.
    # TODO: This could cause issues when only some of the pairs have been
    # written.
    return ['_'.join(db.get_pdb_name(x).split('_')[:-1]) for x in filenames]


def complex_to_pairs(complex, get_pairs, output_dir):
    pairs_txt = output_dir + '/pairs.txt'
    name = complex.name
    logging.info("Working on {:}".format(name))
    pairs, num_subunits = get_pairs(complex)
    logging.info("For complex {:} found {:} pairs out of {:} chains"
                 .format(name, len(pairs), num_subunits))
    sub_dir = output_dir + '/' + db.get_pdb_code(name)[1:3]
    f = name
    if ('mut' in f) and ('mut' not in db.get_pdb_code(name)):
        pdb = db.get_pdb_code(name)(f) + f[f.rfind('_') + 1: f.find('.')]
        sub_dir = output_dir + '/' + pdb
    with sem:
        if len(pairs) > 0:
            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)
            with open(pairs_txt, 'a') as f:
                f.write(name + '\n')

    for i, pair in enumerate(pairs):
        output_dill = "{:}/{:}_{:}.dill".format(sub_dir, name, i)
        write_pair_as_dill(pair, output_dill)


def write_pair_as_dill(pair, output_dill):
    """Write pair as dill file."""
    with open(output_dill, 'wb') as f:
        dill.dump(pair, f)


def read_pair_from_dill(input_dill):
    """Read pair from dill file."""
    with open(input_dill, 'rb') as f:
        return dill.load(f)


def build_get_pairs(type, unbound, nb_fn, full):
    def get_pairs_param(complex):
        return get_pairs(complex, type, unbound, nb_fn, full)

    return get_pairs_param


def get_pairs(complex, type, unbound, nb_fn, full):
    """
    Get pairings for provided complex.

    A complex is a set of chains.  For our interface prediction problem, we
    currently only deal with pairs of chains.  Here, we find all possible such
    pairings, for a given definition of neighboring.
    """
    if type == 'rcsb':
        pairs, num_subunits = \
            _get_rcsb_pairs(complex, unbound, nb_fn, full)
    elif type == 'db5' or type == 'db5mut' or type == 'hotspot':
        pairs, num_subunits = \
            _get_db5_pairs(complex, unbound, nb_fn, full)
    elif type == 'dockground':
        pairs, num_subunits = \
            _get_db5_pairs(complex, unbound, nb_fn, full)
    else:
        raise RuntimeError("Unrecognized dataset type {:}".format(type))
    return pairs, num_subunits


def _get_rcsb_pairs(complex, unbound, nb_fn, full):
    """
    Get pairs for rcsb type complex.

    For this type of complex, we assume that each chain is its own entity,
   and that two chains form a pair if at least one pair of residues spanning
    the two are considered neighbors.
    """
    if unbound:
        logging.error("Requested unbound pairs from RCSB type complex, "
                      "even though they don't have unbound data.")
        raise RuntimeError("Unbound requested for RCSB")
    (pkl_filename,) = complex.bound_filenames
    df = pd.read_pickle(pkl_filename)
    # TODO: Allow for keeping more than just first model.
    if df.shape[0] == 0:
        return [], 0
    df = df[df['model'] == df['model'][0]]
    pairs, num_chains = _get_all_chain_pairs(complex, df, nb_fn, pkl_filename, full)
    return pairs, num_chains


def _get_db5_pairs(complex, unbound, nb_fn, full):
    """
    Get pairs for docking benchmark 5 type complex.

    For this type of complex, we assume that each file is its own entity,
    and that there is essentially one pair for each complex, with one side
    being all the chains of the ligand, and the other all the chains of the
    receptor.
    """
    (lb, rb) = complex.bound_filenames
    (lu, ru) = complex.unbound_filenames
    lb_df = pd.read_pickle(lb)
    rb_df = pd.read_pickle(rb)
    # Always use bound to get neighbors...
    lpos_atoms, rpos_atoms = nb_fn(lb_df, rb_df)
    if unbound:
        # ...but if unbound, we then use the actual atoms from unbound.
        ldf, rdf = pd.read_pickle(lu), pd.read_pickle(ru)

        # Convert atoms' pdb_names to unbound.
        lpos_atoms['pdb_name'] = lpos_atoms['pdb_name'].map(
            lambda x: ca.find_of_type(
                x, ldf['pdb_name'].to_numpy(), None, False, style='db5'))
        rpos_atoms['pdb_name'] = rpos_atoms['pdb_name'].map(
            lambda x: ca.find_of_type(
                x, rdf['pdb_name'].to_numpy(), None, False, style='db5'))

        # Remove residues that we cannot map from bound structure to unbound.
        lres_index = lpos_atoms[['pdb_name', 'model', 'chain', 'residue']]
        rres_index = rpos_atoms[['pdb_name', 'model', 'chain', 'residue']]
        ldf_index = ldf[['pdb_name', 'model', 'chain', 'residue']]
        rdf_index = rdf[['pdb_name', 'model', 'chain', 'residue']]
        lgone = [i for i, x in lres_index.iterrows()
                 if not (np.array(x) == ldf_index).all(1).any()]
        rgone = [i for i, x in rres_index.iterrows()
                 if not (np.array(x) == rdf_index).all(1).any()]
        gone = list(set(lgone).union(set(rgone)))
        if len(gone) > 0:
            logging.warning(
                "Dropping {:}/{:} residues from {:} that didn't map "
                "to unbound from bound.".format(len(gone), len(lpos_atoms), complex.name))
            lpos_atoms = lpos_atoms.drop(gone)
            rpos_atoms = rpos_atoms.drop(gone)

        lsrc, rsrc = lu, ru
    else:
        ldf, rdf = lb_df, rb_df
        lsrc, rsrc = lb, rb

    pos_heavy_atom_idxs = np.stack((lpos_atoms['aid'], rpos_atoms['aid'])).T  # Concatenate structures' labels
    srcs = {'src0': lsrc, 'src1': rsrc}
    pair = Pair(complex=complex.name, df0=ldf, df1=rdf, pos_heavy_atom_idxs=pos_heavy_atom_idxs, srcs=srcs, id=0)
    return [pair], 2


def _get_all_chain_pairs(complex, df, nb_fn, filename, full):
    """Get all possible chain pairs from provided dataframe."""

    pairs = []
    # We reset the index here so each's chain dataframe can be treated
    # independently.
    groups = [(x[0], x[1].reset_index(drop=True))
              for x in df.groupby(['chain', 'model'])]
    num_chains = len(groups)
    num_pairs = 0
    pair_idx = 0
    for i in range(num_chains):
        (chain0, df0) = groups[i]
        for j in range(i + 1, num_chains):
            (chain1, df1) = groups[j]
            pos_atoms0, pos_atoms1 = nb_fn(df0, df1)
            if len(pos_atoms0) == 0:
                # No neighbors between these 2 chains.
                continue
            else:
                num_pairs += 1
            pos_heavy_atom_idxs = np.stack((pos_atoms0['aid'], pos_atoms1['aid'])).T  # Concatenate structures' labels
            srcs = {'src0': filename, 'src1': filename}
            pair = Pair(complex=complex.name, df0=df0, df1=df1,
                        pos_heavy_atom_idxs=pos_heavy_atom_idxs, srcs=srcs, id=pair_idx)
            pairs.append(pair)
            pair_idx += 1
    return pairs, num_chains
