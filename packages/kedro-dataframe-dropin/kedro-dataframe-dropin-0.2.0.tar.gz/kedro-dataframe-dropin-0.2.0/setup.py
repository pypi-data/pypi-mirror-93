# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['kedro_dataframe_dropin']

package_data = \
{'': ['*']}

install_requires = \
['kedro>=0.17.0,<0.18.0', 'pandas>=1.0,<2.0']

setup_kwargs = {
    'name': 'kedro-dataframe-dropin',
    'version': '0.2.0',
    'description': 'A Kedro plugin to utilise pandas dropins (like cuDF or modin) in place of the pandas datasets',
    'long_description': '![logo](static/logo.png)\n\n# kedro-dataframe-dropin\n\n![github-action](https://github.com/mzjp2/kedro-dataframe-dropin/workflows/Lint%20and%20test/badge.svg)\n![code-style](https://img.shields.io/badge/code%20style-black-000000.svg)\n![license](https://img.shields.io/badge/License-MIT-green.svg)\n\n## How do I get started?\n\n```bash\n$ pip install kedro-dataframe-dropin --upgrade\n```\n\n## Then what?\n\nReplace your `pandas.*DataSet` in your `catalog.yml` with\n\n```\nkedro_dataframe_dropin.[rapids|modin].*DataSet\n```\n\nand reap the benefits, as long as your node and pipeline code is compatible with the `cudf`/`modin` API (that tries to replicate `pandas` as much as possible) and your data format is supported by the respective libraries (for example, `cudf` doesn\'t support the `read_excel` method)\n## What is kedro-dataframe-dropin?\n\nkedro-dataframe-dropin is a Kedro plugin that provides modified versions of the `pandas.*` dataset definitions (e.g `pandas.CSVDataSet`) from Kedro, where each dataset has been replaced with one of `pandas` drop-in replacements.\n\nFor example `kedro_dataframe_dropin.modin.CSVDataSet` replicates `pandas.CSVDataSet` but with the `modin.pandas` package replacing `pandas`. Likewise, `kedro_dataframe_dropin.rapids.CSVDataSet` provides a `cuDF`-backed version of the `CSVDataSet`.\n\n## Why does this exist?\n\nThere might be several reasons why you\'d want to consider a drop-in replacement for Pandas. The use-cases are outlined in various places, such as: the [modin documentation](http://modin.readthedocs.io) or [the RAPIDS website](https://rapids.ai).\n\nHowever, the only dataframe-backed datasets that Kedro has out of the box are the `pandas` and `pyspark` ones. If you wanted to use, say, a `modin` dataframe backed by `Dask` or `Ray`, you\'d need to write a [custom dataset](https://kedro.readthedocs.io/en/stable/07_extend_kedro/03_custom_datasets.html) for each file format (`.csv`, `.xls`, etc...).\n\nThis lets you swap out your `catalog.yml` from:\n\n```yaml\n# conf/base/catalog.yml [before]\nrockets:\n    type: pandas.CSVDataSet\n    filepath: data/01_raw/rockets.csv\n\nreviews:\n    type: pandas.ExcelDataSet\n    filepath: data/01_raw/reviews.xslsx\n```\n\nto:\n\n```yaml\n# conf/base/catalog.yml [after]\nrockets:\n    type: kedro_dataframe_dropin.rapids.CSVDataSet\n    filepath: data/01_raw/rockets.csv\n\nreviews:\n    type: kedro_dataframe_dropin.modin.ExcelDataSet\n    filepath: data/01_raw/reviews.xlsx\n```\n\nand as long as the code within your nodes fits within `modin` or `cudf`\'s implementation of a subset of the `pandas` API, you\'ll be done!\n\n## What dropins are currently supported?\n\n| dropin       | supported |\n| ------------ | --------- |\n| modin[ray]   | ✅        |\n| modin[dask]  | ✅        |\n| cudf         | ✅        |\n| dask         | \U0001f7e0        |\n| dask-cudf    | \U0001f7e0        |\n\n✅: compatible\n\U0001f7e0: No kedro versioning and some datasets (like `SQLTableDataSet`) don\'t work despite being available on both `kedro` and the drop-in.\n\n## What happens when Kedro adds or changes a `pandas` dataset?\n\nThe beauty of it is that this will stay in complete sync with Kedro\'s `pandas.*` library without any code changes or releases required. It\'s implemented through hot-swapping the `pandas` module with one of the replacements you specified.\n\n## Examples\n\nAs an example of why you might want to use this, here are the results of some very rough and preliminary benchmarking. These were conducted on a Google Colaboratory notebook (thanks Google!) with a Tesla T4 GPU and a 2-core CPU. The data used was a 5 million row CSV, weighing in at around a 100mb downloaded from [here](http://eforexcel.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/).\n\n```\n# base/conf/catalog.yml\ncudf:\n  type: kedro_dataframe_dropin.rapids.CSVDataSet\n  filepath: data/01_raw/data.csv\n\npandas:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/data.csv\n```\n\nUsing the two datasets within the `kedro ipython` console shows a world of difference, with reading the file in being 10x faster, doing a groupby being 6x faster and taking the mean being 5x faster.\n\nThis helps shorten:\n\n* The feedback loop when prototyping and exploring your data within a `kedro ipython` or a `kedro jupyter` session\n* The feedback loop when running your pipelines in development and debugging/experimenting with various different methodologies\n* Your production runtime\n\n```\nIn [1]: %timeit gdf = catalog.load("cudf")\n702 ms ± 7.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [2]: %timeit df = catalog.load("pandas")\n8.22 s ± 101 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [3]: %timeit gdf.groupby("Region")\n4.75 µs ± 56.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nIn [4]: %timeit df.groupby("Region")\n26.7 µs ± 397 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\nIn [5]: %timeit df["Total Revenue"].mean()\n11.8 ms ± 87.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nIn [6]: %timeit gdf["Total Revenue"].mean()\n2.71 ms ± 31.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nAny additional benchmarks you do and want to share back would be much appreciated. Feel free to open an issue!\n\n## Some special notes on RAPIDS\n\n### The rest of the `cu*` ecosystem\n\nYour data processing step gets faster (assuming you have the right conditions) by plugging in the `cudf` module from RAPIDs in place of `pandas`, but it doesn\'t end there.\n\nYou can continue to make use of your GPU speedup in the rest of your pipeline lifecycle (predictions, ML, graph, etc...) by using the rest of the `cuda` ecosystem of tools (`cuML` and the ilk) in place of tools like `sklearn`.\n\n### Why are some data formats missing?\n\nWith the way this plugin was designed, it only hot swaps in `cudf` in place of `pandas` where the Kedro pandas dataset exists.\n\nSo as it stands today, with the Kedro codebase not having an `ORCDataSet` for example, this plugin won\'t have it either. You\'ll need to build your own custom own.\n\nOr better yet, head over to the [Kedro](https;//github.com/quantumblacklabs/kedro) codebase and contribute the `pandas` version of it to their codebase. This plugin will then automatically pick it up and provide a `cudf`-equivalent.\n\n## Some special notes on `dask-cuDF` and `dask`\n\nNote that `dask` and `dask-cuDF` will delay compute and operations across nodes are actually building up a computation graph. They will be parallelised across your CPU/GPU when you invoke a `.compute()` operation (like `len` or save it to disk by having its output be a non-memory dataset in the catalog).\n\nNote that Kedro versioning won\'t be possible with these datasets, since Kedro completely owns the I/O and simply passes the file handle down to `dask`/`dask-cuDF` which doesn\'t accept it - since file handles can\'t be shared across (CPU or GPU) workers. Instead what we do is extract the filepath and pass it to `dask` who also use `fsspec` and so you still have full remote-layer interopability with the benefit of parallelised compute.\n\nConsider giving Matthew Rocklin\'s [blog post on `dask-cuDF`](http://matthewrocklin.com/blog/2019/01/13/dask-cudf-first-steps) and the philsophy of it simply being a different "engine" for `dask.DataFrame` a read.\n\n## Caveats\n\nKeep in mind that in order to remain consistent with the adage of not copying memory, when passing these dataframes between nodes, they _will not_ be copied - but simply passed through as the same underlying Python object, so if you\'re doing mutable operations on them across different nodes, keep in that in mind.\n',
    'author': 'Zain Patel',
    'author_email': 'zain.patel06@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/mzjp2/kedro-dataframe-dropin',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.7.1,<3.9',
}


setup(**setup_kwargs)
