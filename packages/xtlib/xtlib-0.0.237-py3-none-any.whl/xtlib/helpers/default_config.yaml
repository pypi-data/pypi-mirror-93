#-------------------------------------------------------------------------------------------------------------------
# DO NOT EDIT:
#   - a new version of this file is released with each XTLib version; it will overwrite this file without warning.
#   - to change a subset of these properties, use the "xt config" to create a LOCAL (current directory) config file.
#-------------------------------------------------------------------------------------------------------------------
# default_config.yaml: default configuration file for XT.  (READONLY)
#    The contents of this file contain the default settings that define how XT operates.  Many of the settings can be 
#    overridden by the various XT command  options (see "xt help commands" for more details). Detailed instructions 
#    for getting started with this config file are available in the "XT Config File" help topic.
#-------------------------------------------------------------------------------------------------------------------

external-services:
    # compute services
    philly: {type: "philly"}
    xtsandboxbatch: {type: "batch", key: "$vault", url: "https://xtsandboxbatch.eastus.batch.azure.com"}

    # AML platform
    aml-sandbox-ws: {type: "aml", subscription-id: "41c6e824-0f66-4076-81dd-f751c70a140b", resource-group: "xt-sandbox"}
    canadav100ws: {type: "aml", subscription-id: "db9fc1d1-b44e-45a8-902d-8c766c255568", resource-group: "canadav100"}
    usscv100ws: {type: "aml", subscription-id: "d4c59fc3-1b01-4872-8981-3ee8cbd79d68", resource-group: "usscv100"}

    # ITP platform  (name on left is k8s VC)
    resrchvc: {type: "itp", subscription-id: "46da6261-2167-4e71-8b0d-f4a45215ce61", resource-group: "researchvc"}
    resrchvc-sc: {type: "itp", subscription-id: "46da6261-2167-4e71-8b0d-f4a45215ce61", resource-group: "researchvc-sc"}
    resrchvc-eus: {type: "itp", subscription-id: "46da6261-2167-4e71-8b0d-f4a45215ce61", resource-group: "researchvc-eus"}
    platform-aml: {type: "itp", subscription-id: "20d3c9e4-625d-45e1-ac8a-def90d3c4a88", resource-group: "RSTrainEastUS_32GB-2-aml"}
    msrhyper: {type: "itp", subscription-id: "46da6261-2167-4e71-8b0d-f4a45215ce61", resource-group: "msrhyperVC"}

    # storage services
    xtsandboxstorage: {type: "storage", provider: "azure-blob-21", key: "$vault"}
    sandboxstorage4atlas: {type: "storage", provider: "azure-blob-21", key: "$vault"}
    sandboxstorage4agent2: {type: "storage", provider: "azure-blob-21", key: "$vault"}
    storage4mongovm: {type: "storage", provider: "azure-blob-21", key: "$vault"}
    sandboxstoragev2: {type: "storage", provider: "azure-blob-21", key: "$vault"}
    sandboxstoragev2s: {type: "storage", provider: "azure-blob-21", key: "$vault"}

    # local storage
    filestorage: {type: "storage", provider: "store-file", path: "~/.xt/file_store"}
    filestorage2: {type: "storage", provider: "store-file", path: "~/.xt/file_store2"}

    # database services
    xt-sandbox-cosmos: {type: "mongo", connection-string: "$vault"}
    atlasmongo: {type: "mongo", connection-string: "$vault"}
    mongoagent2: {type: "mongo", connection-string: "$vault"}
    mongovm: {type: "mongo", connection-string: "$vault"}
    xtsandboxmongov2: {type: "mongo", connection-string: "$vault"}
    xtsandboxsql2: {type: "odbc", connection-string: "$vault"}
    xtsandboxsql3: {type: "odbc", connection-string: "$vault"}

    # vault services
    sandbox-vault: {type: "vault", secret-name: "xt-keys", url: "https://xtsandboxvault.vault.azure.net/"}
    sandbox-vault-v2: {type: "vault", secret-name: "xt-keys", url: "https://xtsandboxkeyvaultv2.vault.azure.net/"}

    # registry services
    xtsandboxregistry: {type: "registry", login-server: "xtsandboxregistry.azurecr.io", username: "xtsandboxregistry", password: "$vault", login: "true"}
    xtcontainerregistry: {type: "registry", login-server: "xtcontainerregistry.azurecr.io", username: "xtcontainerregistry", password: "$vault", login: "true"}
    philly-registry: {type: "registry", login-server: "phillyregistry.azurecr.io", login: "false"}
    docker-hub: {type: "registry", login-server: "docker.io", login: "false"}

store: "sandbox_v1"

stores:
    sandbox_v1: {storage: "xtsandboxstorage", database: "xt-sandbox-cosmos", vault: sandbox-vault, target: "local"}
    sandbox_v2: {storage: "sandboxstoragev2", database: "xtsandboxmongov2", vault: sandbox-vault-v2, target: "local"}
    sandbox_v3: {storage: "sandboxstoragev2s", database: "xtsandboxsql2",  vault: sandbox-vault-v2, target: "local"}
    #sandbox_sql: {storage: "sandboxstoragev2s", database: "xtsandboxsql3",  vault: sandbox-vault-v2, target: "local"}

compute-targets:
    # Azure VM size cheat sheet:
        # ND40RS_V2: 8xV100 (32 GB)
        # NC24RS_V3: V100 (16 GB)
        # NC-V3: V100 (16 GB)

        # NC-series: K80 (12 GB)
        # NC-V2: P100 (16 GB)
        # ND-series: P40 (24 GB)
        # NV-xxx: M60 (8 GB) (bad news)

    # BATCH targets
    batch: {service: "xtsandboxbatch", vm-size: "Standard_NC6", azure-image: "dsvm", nodes: 1, low-pri: true,  
        docker: "pytorch-xtlib", setup: "batchd"}

    # AML targets

    # Tesla K80 (12 GB)
    aml: {service: "aml-sandbox-ws", compute: "sandbox-compute", nodes: 1, low-pri: false, docker: "pytorch-xtlib", setup: "amld"} 
    aml4x: {service: "aml-sandbox-ws", compute: "compute4x", nodes: 1, low-pri: false, docker: "pytorch-xtlib", setup: "amld"} 

    # V100, 16GB (?)
    canada: {service: "canadav100ws", compute: "canada1GPUcl", nodes: 1, low-pri: false, docker: "pytorch-xtlib", setup: "amld"}
    canada4x: {service: "canadav100ws", compute: "canadav100cl", nodes: 1, low-pri: false, docker: "pytorch-xtlib", setup: "amld"}
    aml-us8x: {service: "usscv100ws", compute: "usscv100cl", nodes: 1, low-pri: false, docker: "pytorch-xtlib", setup: "amld"}

    #---- ITP targets ----

    # P40, 24 GB
    itp-eus-p40-24gb: {service: "resrchvc-eus", compute: "itpeusp40cl", sku: "g1", nodes: 1, low-pri: true, setup: "itpd", docker: "pytorch-xtlib"}  # MSR-Azure-EastUS-P40
    itp-eus-p40: {service: "platform-aml", compute: "eus-p40", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}

    # P100, 16 GB
    itp-eus-p100-16gb: {service: "resrchvc-eus", compute: "itpeusp100cl", sku: "g1", nodes: 1, low-pri: true, setup: "itpd", docker: "pytorch-xtlib"}  # MSR-Azure-EastUS-P100
    itp-rl1-p100-16gb:  {service: "resrchvc", compute: "itplabrl1cl1", sku: "g1", nodes: 1, low-pri: true, setup: "itpd", docker: "pytorch-xtlib"}  # MSR-Lab-RL1

    # V100, 16GB
    itp: {service: "resrchvc", compute: "itpeastusv100cl", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-us: {service: "resrchvc", compute: "itpeastusv100cl", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-asia: {service: "resrchvc", compute: "itpseasiav100cl", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-eus1: {service: "platform-aml", compute: "v100-8x-eus-1", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-eus2: {service: "platform-aml", compute: "v100-8x-eus", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-scu-v100-16gb: {service: "resrchvc-sc", compute: "itpscusv100cl", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    
    # V100, 32GB
    itp-rr1: {service: "resrchvc", compute: "itplabrr1cl1", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}
    itp-hyper: {service: "msrhyper", compute: "itphyperdgxcl1", sku: "g1", nodes: 1, low-pri: false, setup: "itpd", docker: "pytorch-xtlib"}


    #----  POOL targets ----
    local-docker: {service: "pool", boxes: ["localhost"], setup: "local", docker: "pytorch-xtlib-local"}

user-filters: 
    dead: {prop: "status", type: "str", op: "=", value: "killed"}

dockers:
    pytorch-xtlib: {image: "rfernand/pytorch-xtlib-cuda10:u18.04_torch_1.7_cuda_10.1_xtlib_235_odbc17"}
    pytorch-xtlib-local: {registry: "", image: "pytorch-xtlib:latest"}

    # legacy docker images    
    pytorch-xtlib-old: {image: "rfernand/pytorch-xtlib:torch_1.6_cuda_10.1_xtlib_230_odbc17"}
    pytorch-xtlib-cuda9: {image: "rfernand/pytorch-xtlib-cuda9:torch_1.6_cuda_9.2_xtlib_230_odbc17"}
    philly-pytorch: {registry: "philly-registry", image: "microsoft_pytorch:v1.2.0_gpu_cuda9.0_py36_release_gpuenv_hvd0.16.2"}
    pytorch-xtlib-reg: {registry: "xtsandboxregistry", image: "pytorch-xtlib:latest"}

azure-batch-images:
    # these are Microsoft-provided VM images that you specify with your azure batch compute targets (see [compute-targets] section above)
    dsvm: {offer: "linux-data-science-vm-ubuntu", publisher: "microsoft-dsvm", sku: "linuxdsvmubuntu", node-agent-sku-id: "batch.node.ubuntu 16.04", version: "latest"}
    ubuntu16: {publisher: "Canonical", offer: "UbuntuServer", sku: "16.04-LTS", node-agent-sku-id: "batch.node.ubuntu 16.04", version: "latest"}
    ubuntu18: {publisher: "Canonical", offer: "UbuntuServer", sku: "18.04-LTS", node-agent-sku-id: "batch.node.ubuntu 18.04", version: "latest"}
    ubuntu-container: {publisher: "microsoft-azure-batch", offer: "ubuntu-server-container", sku: 16-04-lts", node-agent-sku-id: "batch.node.ubuntu 16.04", version: "latest"}
    
setups:
    batchd: {pip-packages: ["xtlib==*"], use-sudo: true, install-blobfuse: false}
    amld: {pip-packages: ["xtlib==*"], use-sudo: true, install-blobfuse: false}

    # workaround ITP which installs torch==1.3.0
    itpd: {pip-packages: ["torch==1.7.0", "xtlib==*"], use-sudo: true, install-blobfuse: false}

    # local machine is assumed to be self-managed (user configured for xtlib and ML app requirements)
    local: {activate: "$call conda activate $current_conda_env", conda-packages: [], pip-packages: []}

    # legacy setups (without docker images)
    batch_native: {activate: "conda activate py36", conda-packages: [], pip-packages: ["xtlib==*"], use-sudo: true}
    aml_native: {activate: null, conda-packages: [], pip-packages: ["xtlib==*"]}

general:
    advanced-mode: false                   # XT now defaults to basic mode
    username: "$username"                  # used to log the user associated with runs created by the current user
    workspace: "ws1"                       # name of current workspace 
    experiment: "exper1"                   # default name of experiment associated with each run
    feedback: true                         # when true, we display progress feedback for file uploads and downloads
    run-cache-dir: "~/.xt/runs-cache"      # where we cache run information (SUMMARY and ALLRUNS)
    distributed: false                     # when true, runs with multiple boxes/nodes are performed in distributed training mode
    direct-run: false                      # when true, the target is run without using the XT controller (Philly, Batch, Azure ML)
    quick-start: false                     # when true, XT start-up time will be reduced (experimental feature)
    env-vars: {}                           # list of name=value pairs, separated by commas that the target app can read at start of run
    authentication: "auto"                 # one of: auto, browser, device-code
    remote-control: false                  # when true, XT client can send query/request commands to the XT controller
    monitor: "bg"                          # should new run be monitored in console?  one of: bg, new, same, none
    max-run-workers: 15                    # max number of bg workers to use for run command

    # TODO: move to a new metrics section
    primary-metric: "test-acc"             # name of metric to optimize in roll-ups, hyperparameter search, and early stopping
    maximize-metric: true                  # how primary metric is aggregated for hp search, hp explorer, early stopping 
    step-name: "step"                      # usually "step" or "epoch"; the name of your app's logged metrics index value

code:
    code-dirs: ["$scriptdir/**"]           # path to the code directories needed for the run (code snapshot)
    code-upload: true                      # upload code to job store before run, for use by ML script
    code-zip: "fast"                       # none/fast/compress ("fast" means zip w/o compression)
    code-omit: [".git", "__pycache__"]     # directories and files to omit when capturing code files
    xtlib-upload: false                    # upload XTLIB sources files for each run and use for controller and ML app
    working-dir: "."                       # specifies the working directory for the run, relative to the code directory

after-files:
    after-dirs: ["output/**"]              # specifies output files (for capture from compute node to STORE)
    after-upload: true                     # should after files be uploaded at end of run?
    after-omit: [".git", "__pycache__"]    # directories and files to omit when capturing after files

data:
    data-local: ""                         # local directory of data for app
    data-upload: false                     # should data automatically be uploaded
    data-share-path: ""                    # path in data share for current app's data
    data-action: "none"                    # data action at start of run: none, download, mount
    data-omit: []                          # directories and files to omit when capturing before/after files
    data-writable: false                   # when true, mounted data is writable

model:
    model-local: ""                        # local directory of model for app
    model-share-path: ""                   # path in model share for current app's model
    model-action: "none"                   # model action at start of run: none, download, mount
    model-writable: false                  # when true, mounted model is writable

database:
    update-job-stats: true                 # when true, job stats are updated as each run begins/ends
    update-run-stats: true                 # when true, run stats are updated as each run begins/ends
    update-node-stats: true                # when true, node stats are updated as each run begins/ends
    
    add-log-records: false                 # the new default: don't write log_records to mongo
    buffer-metrics: 0                      # limit write of flattened metrics to database to every N mins
    max-log-workers: 50                    # max number of background workers to use when reading log records from storage
    max-run-delay: 0                       # max secs each run will be delayed on start (smooth out database usage on large jobs)
    fake-error-percent: 0                  # set to .15, for example, to fail 15% of all dbx calls (for stress testing)
    chunk-size: 50                         # number of rows per chunk, when splitting large query results among bg workers    

    max-retries: 25                        # max number of times to retry errors (like "request rate too large")
    max-backoff: 30                        # max number of seconds to wait between error retries
    reset-connection: false                # when true, new connection is attempted before any error is retried
    extended-logging: false                 # when true, all ODBC calls and errors are logged to console

logging:
    mirror-files: "logs/**"                # default wildcard path for log files to mirror
    mirror-dest: "storage"                 # one of: none, storage
    log: true                              # specifies if experiments are logged to STORE
    notes: "none"                          # control when user is prompted for notes (none, before, after, all)
    capture-setup-cmds: true               # during node setup, selected commands will have their output sent to a log file
    pip-freeze: false                      # should 'pip freeze' be run during node setup process (logging before/after pip packages)
    log-reports: true                      # should GPU, CPU, and MEMORY reports be logged during setup
    merge-batch-logs: false                # when true, STDOUT.txt and STDERR.txt are merged into a STDBOTH.txt file

internal:
    console: "normal"                      # controls the level of console output (none, normal, diagnostics, detail)
    stack-trace: false                     # show stack trace for errors  
    auto-start: false                      # when true, the XT controller is automatically started on 'status' cmd

aml-options:
    use-gpu: true                          # use GPU(s) 
    framework: "none"                      # we support pytorch, tensorflow, chainer, or none (ignored if using custom docker image)
    fw-version: ""                         # version of framework (string) (ignored if using custom docker image)
    # user-managed: false                  # when true, AML assumes we have correct prepared environment (for local runs)
    distributed-training: "mpi"            # one of: mpi, gloo, or nccl
    max-seconds: null                      # max secs for run before timeout 

early-stopping:
    early-policy: "none"           # bandit, median, truncation, none
    delay-evaluation: 10           # number of evals (metric loggings) to delay before the first policy application
    evaluation-interval: 1         # the frequencency (# of metric logs) for testing the policy
    slack-factor: 0                # (bandit only) specified as a ratio, the delta between this eval and the best performing eval
    slack-amount: 0                # (bandit only) specified as an amount, the delta between this eval and the best performing eval
    truncation-percentage: 5       # (truncation only) percent of runs to cancel at each eval interval

hyperparameter-search:
    option-prefix: "--"                 # prefix used by ML app for options specified on the cmd line (set to "$none" to disable parsing/generation of options for hp search)
    aggregate-dest: "job"               # set to "job", "experiment", or "none"
    search-type: "random"               # random, grid, bayesian, or dgd
    static-search: true                 # perform hparameter searches at job submit time, when possible
    max-minutes: null                   # max minutes before terminating search
    hp-config: ""                       # the name of the text file containing the hyperparameter ranges to be searched
    fn-generated-config: "config.yaml"  # name of runset file generated by dynamic hyperparameter search
    concurrent: 1                       # max number of concurrent runs per node
    max-runs: null                      # used to limit total search runs in a full/grid search 

hyperparameter-explorer:
    hx-cache-dir: "c:/hx_cache"        # directory hx uses for caching experiment runs 
    steps-name: "steps"                # usually "epochs" or "steps" (hyperparameter - total # of steps to be run)
    log-interval-name: "LOG_INTERVAL"  # name of hyperparameter that specifies how often to log metrics
    time-name: "sec"                   # usually "epoch" or "sec
    sample-efficiency-name: "SE"       # sample efficiency name 
    success-rate-name: "RSR"           # success rate name 

plots:
    highlight: ""                  # set to "$alive" to highlight alive runs/jobs/experiments/nodes
    color-highlight: ""            # set to bold, italic, or a matplotlib color

run-reports:
    sort: "name"                   # default column sort for run reports (e.g., name, value, status, duration, etc.)
    group: ""                      # column used to group the run reports
    number-groups: false           # if true, group names will be preceeded by a group number (based on sorting order)
    reverse: false                 # if experiment sort should be reversed in order    
    max-width: 30                  # max width of any column
    precision: 3                   # number of fractional digits to display for float values
    significance: 2                # number of significant digits to display (by increasing precision, when needed)
    max-fixed-length: 7            # maximum # of fractional digits to display before using scientific notation
    count: false                   # returns # of records being retrieved at top of report (slows down start of report)

    # report colors:
    #    underline, negative, black, red, green, yellow, blue, magenta, cyan, gray, 
    #    lightred, lightgreen, lightyellow, lightblue, lightmagenta, lightcyan, darkgray, white

    color-hdr: ""                  # color to use when printing header rows
    color-highlight: negative      # color to use when printing highlighted rows
    highlight: ""                  # use "$alive" to highlight alive runs

    uppercase-hdr: true            # show column names in uppercase letters
    right-align-numeric: true      # right align columns that contain int/float values
    truncate-with-ellipses: true   # if true, "..." added at end of truncated column headers/values
    status: ""                     # the status values to match for 'list runs' cmd
    report-rollup: false           # if primary metric is used to select run metrics to report (vs. last set of metrics)
    last: 10                       # default number of runs to show

    # "columns" defines the columns to show (and their order) for the "list runs" cmd.  The columns listed 
    # should be a standard column, or a user-logged hyperparameter or metric.  use "list runs --available" to find available columns.
    columns: ["run", "created:$do", "experiment", "queued", "job", "target", "repeat", "search", "status", 
        "tags.priority", "tags.description",
        "hparams.lr", "hparams.momentum", "hparams.optimizer", "hparams.steps", "hparams.epochs",
        "metrics.step", "metrics.epoch", "metrics.train-loss", "metrics.train-acc", 
        "metrics.dev-loss", "metrics.dev-acc", "metrics.dev-em", "metrics.dev-f1", "metrics.test-loss", "metrics.test-acc", 
        "duration", 
        ]

commands: []                       # array of run commands
    
job-reports:
    sort: "name"                   # default column sort for experiment list (name, value, status, duration)
    reverse: false                 # if experiment sort should be reversed in order    
    max-width: 30                  # max width of any column
    precision: 3                   # number of fractional digits to display for float values
    significance: 2                # number of significant digits to display (by increasing precision, when needed)
    max-fixed-length: 7            # maximum # of fractional digits to display before using scientific notation
    uppercase-hdr  : true          # show column names in uppercase letters
    right-align-numeric: true      # right align columns that contain int/float values
    truncate-with-ellipses: true   # if true, "..." added at end of truncated column headers/values
    last: 10                       # default number of jobs to show

    # "columns" defines the columns to show (and their order) for the "list jobs" cmd.  The columns listed 
    # should be a standard column.  use "list jobs --available" to find available columns.
    columns: ["job", "created", "started", "workspace", "experiment", "target", "nodes", "repeat", "tags.description", "tags.urgent", "tags.sad=SADD", "tags.funny", "low_pri", 
        "vm_size", "azure_image", "service", "vc", "cluster", "queue", "service_type", "search", 
        "job_status:$bz", "running_nodes:$bz", "running_runs:$bz", "error_runs:$bz", "completed_runs:$bz"]

node-reports:
    sort: "name"                   # default column sort for experiment list (name, value, status, duration)
    reverse: false                 # if experiment sort should be reversed in order    
    max-width: 30                  # max width of any column
    precision: 3                   # number of fractional digits to display for float values
    significance: 2                # number of significant digits to display (by increasing precision, when needed)
    max-fixed-length: 7            # maximum # of fractional digits to display before using scientific notation
    uppercase-hdr  : true          # show column names in uppercase letters
    right-align-numeric: true      # right align columns that contain int/float values
    truncate-with-ellipses: true   # if true, "..." added at end of truncated column headers/values
    last: 10                       # default number of jobs to show

    # "columns" defines the columns to show (and their order) for the "list nodes" cmd.  The columns listed 
    # should be a standard column.  use "list nodes --available" to find available columns.
    columns: ["name", "job", "target", "queued", "workspace", "status", "restarts:$bz", 
              "running_nodes:$bz", "running_runs:$bz", "error_runs:$bz", "completed_runs:$bz", "total_runs",
              "duration"]

tensorboard:
    template: "{workspace}_{run}_{logdir}"

script-launch-prefix:
    # list cmds used to launch scripts (controller, run, parent), by box-class
    windows: ""
    linux: "bash --login"
    dsvm: "bash --login"
    azureml: "bash"
    itp: "bash"
    philly: "bash --login"  

boxes:
    # This section lets you define remote computers for running your experiments (samples listed below).
    # REQUIREMENTS: each box needs to have ports 22 and 18861 open for incoming messages.
    # The "actions" property is a list of store names ("data", "model") whose download or mount actions should be performed on the box.
    local: {address: "localhost", max-runs: 1, actions: [], setup: "local"}

providers:
    # this is an user-extensible set of providers for XT commands, the backend compute services,
    # the hyperparameter search services, and storage services.

    command: {
        "compute": "xtlib.impl_compute.ImplCompute", 
        "storage": "xtlib.impl_storage.ImplStorage", 
        "help": "xtlib.impl_help.ImplHelp", 
        "utility": "xtlib.impl_utilities.ImplUtilities"
    }

    compute: {
        "pool": "xtlib.backends.backend_pool.PoolBackend", 
        "philly": "xtlib.backends.backend_philly.Philly",
        "batch": "xtlib.backends.backend_batch.AzureBatch",
        "aml": "xtlib.backends.backend_aml.AzureML",
        "itp": "xtlib.backends.backend_itp.ITP"
    }

    hp-search: {
        "dgd": "xtlib.hparams.hp_search_dgd.DGDSearch",
        "bayesian": "xtlib.hparams.hp_search_bayesian.BayesianSearch",
        "random": "xtlib.hparams.hp_search_random.RandomSearch"
    }

    storage: {
        "azure-blob-21": "xtlib.storage.store_azure_blob21.AzureBlobStore21",
        "azure-blob-210": "xtlib.storage.store_azure_blob210.AzureBlobStore210",
        "store-file": "xtlib.storage.store_file.FileStore",
    }
